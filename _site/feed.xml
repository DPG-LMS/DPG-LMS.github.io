<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-10-04T22:20:38-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">DPG-LMS Project</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><entry><title type="html">SpeakQL2: A Dialect System for Improving Speech-driven Querying of Structured Data</title><link href="http://localhost:4000/2022/09/30/kyle.html" rel="alternate" type="text/html" title="SpeakQL2: A Dialect System for Improving Speech-driven Querying of Structured Data" /><published>2022-09-30T00:00:00-05:00</published><updated>2022-09-30T00:00:00-05:00</updated><id>http://localhost:4000/2022/09/30/kyle</id><content type="html" xml:base="http://localhost:4000/2022/09/30/kyle.html"><![CDATA[<p>SpeakQL2 builds upon prior work done within the ADALab on a speech + touch SQL query interface designed to enable effective SQL querying against databases using mobile devices. SpeakQL2 introduces a new dialect of SQL designed to make the speech dictation process more natural. In this presentation, we discuss the objectives of the language, language features, implementation details, and an ongoing usability study that we are conducting to evaluate various features of the new dialect.</p>

<p>Speaker bio:</p>

<p>Kyle Luoma is a PhD student in the UCSD Jacobs School of Computer Science and Engineering (CSE) database lab. He is also a member of the United States Army, and is pursuing his PhD as part of his pathway toward a follow-on position as research faculty at the Army Cyber Institute, United States Military Academy at West Point. Kyle is currently conducting research in the area of human-database interaction and database usability.</p>]]></content><author><name>Kyle Luoma (UC San Diego)</name></author><summary type="html"><![CDATA[SpeakQL2 builds upon prior work done within the ADALab on a speech + touch SQL query interface designed to enable effective SQL querying against databases using mobile devices. SpeakQL2 introduces a new dialect of SQL designed to make the speech dictation process more natural. In this presentation, we discuss the objectives of the language, language features, implementation details, and an ongoing usability study that we are conducting to evaluate various features of the new dialect.]]></summary></entry><entry><title type="html">DataPrep: Accelerate Data Preparation for AI</title><link href="http://localhost:4000/2021/12/01/jiannan.html" rel="alternate" type="text/html" title="DataPrep: Accelerate Data Preparation for AI" /><published>2021-12-01T00:00:00-06:00</published><updated>2021-12-01T00:00:00-06:00</updated><id>http://localhost:4000/2021/12/01/jiannan</id><content type="html" xml:base="http://localhost:4000/2021/12/01/jiannan.html"><![CDATA[<p>Data scientists have been complaining about data preparation (data collection –&gt; data understanding –&gt; data cleaning –&gt; data enrichment –&gt; data integration –&gt; feature engineering) for many years. Although some efforts have been devoted to solving this problem, a recent survey released by Anaconda in 2020 shows that it is still the case that “Data preparation and cleansing takes valuable time away from real data science work and has a negative impact on overall job satisfaction.” Most recently, Andrew Ng urged the AI community to shift from Model-Centric toward Data-Centric AI development.</p>

<p>In this talk, I will start by answering two fundamental questions: i) what makes data preparation hard? ii) why has this problem not been solved? Then, I will present DataPrep (http://dataprep.ai), a fast and easy-to-use python library to address these challenges.  The DataPrep library currently contains three components: a data connector component to simplify and accelerate data collection, an exploratory data analysis (EDA) component to enable fast data understanding, and a data cleaning component to clean and standardize data. I will describe their novel design and demonstrate how they can significantly save data scientists’ time. In the end, I will share some lessons and experiences that I learned about open-source software development.</p>

<p>Speaker bio:</p>

<p>Jiannan Wang (https://www.cs.sfu.ca/~jnwang) is an Associate Professor and the Director of the Professional Master’s Program in the School of Computing Science at Simon Fraser University. Prior to that, he was a postdoc in the AMPLab at UC Berkeley. He obtained his Ph.D. from Tsinghua University. He has over ten years’ research experience in data preparation.  His research contributions won him the VLDB Best Experiments, Analysis &amp; Benchmark Paper Award (2021), a CS-Can/Info-Can Outstanding Early Career Researcher Award (2020), an IEEE TCDE Rising Star Award (2018), an ACM SIGMOD Best Demonstration Award (2016), a Distinguished Dissertation Award from the China Computer Federation (2013), and a Google Ph.D. Fellowship (2011). He is a General Co-chair for VLDB 2023, a Ph.D. Symposium Track Chair for ICDE 2022, an Associate Editor for VLDB 2021, and a core PC member for SIGMOD 2019.</p>

<p>Public video of talk: https://www.youtube.com/watch?v=M3xO0vVIKV0</p>]]></content><author><name>Dr. Jiannan Wang (Simon Fraser University)</name></author><summary type="html"><![CDATA[Data scientists have been complaining about data preparation (data collection –&gt; data understanding –&gt; data cleaning –&gt; data enrichment –&gt; data integration –&gt; feature engineering) for many years. Although some efforts have been devoted to solving this problem, a recent survey released by Anaconda in 2020 shows that it is still the case that “Data preparation and cleansing takes valuable time away from real data science work and has a negative impact on overall job satisfaction.” Most recently, Andrew Ng urged the AI community to shift from Model-Centric toward Data-Centric AI development.]]></summary></entry><entry><title type="html">Efficient and Reliable Query Processing using Machine Learning</title><link href="http://localhost:4000/2021/11/17/daniel.html" rel="alternate" type="text/html" title="Efficient and Reliable Query Processing using Machine Learning" /><published>2021-11-17T00:00:00-06:00</published><updated>2021-11-17T00:00:00-06:00</updated><id>http://localhost:4000/2021/11/17/daniel</id><content type="html" xml:base="http://localhost:4000/2021/11/17/daniel.html"><![CDATA[<p>Given the rise of increasingly powerful models, machine learning (ML) can now be used to answer a range of queries over unstructured data (e.g., videos, text) by extracting structured information over this data (e.g., object types and positions in scenes). Unfortunately, ML can be prohibitively expensive to deploy (e.g., costing millions of dollars to process a year’s worth of data) and too unreliable for many use cases.</p>

<p>In this talk, I will describe how a new class of ML-based query systems can tackle these challenges. I will first describe how query optimization systems can enable ML-based queries over unstructured data. Given an “oracle” method the user wishes to mimic (i.e., an human expert labeler or an expensive ML method), systems can automatically train and execute more inexpensive “proxy” models that approximate the oracle. I will describe our work building query systems to accelerate a range of queries – including common selection, aggregation, and limit operations – by orders of magnitude while providing strong statistical guarantees on query accuracy despite using possibly noisy proxy-based approximations. Towards improving the reliability of ML systems, I will also describe new systems-based programming abstractions to find errors in both ML models and in human-generated labels. Finally, I will discuss our experience deploying these systems in the field both at Toyota and with Stanford ecologists.</p>

<p>Speaker bio:</p>

<p>Daniel Kang is a final year PhD student in the Stanford DAWN lab, co-advised by Professors Peter Bailis and Matei Zaharia. His research focuses on systems approaches for deploying unreliable and expensive machine learning methods efficiently and reliably. In particular, he focuses on using cheap approximations to accelerate query processing algorithms and new programming models for ML data management. Daniel is collaborating with autonomous vehicle companies and ecologists to deploy his research. His work is supported in part by the NSF GRFP and the Google PhD fellowship.</p>]]></content><author><name>Daniel Kang (Stanford University)</name></author><summary type="html"><![CDATA[Given the rise of increasingly powerful models, machine learning (ML) can now be used to answer a range of queries over unstructured data (e.g., videos, text) by extracting structured information over this data (e.g., object types and positions in scenes). Unfortunately, ML can be prohibitively expensive to deploy (e.g., costing millions of dollars to process a year’s worth of data) and too unreliable for many use cases.]]></summary></entry><entry><title type="html">A 3-year History of Instance Optimized DB Research at Microsoft</title><link href="http://localhost:4000/2021/11/03/umar.html" rel="alternate" type="text/html" title="A 3-year History of Instance Optimized DB Research at Microsoft" /><published>2021-11-03T00:00:00-05:00</published><updated>2021-11-03T00:00:00-05:00</updated><id>http://localhost:4000/2021/11/03/umar</id><content type="html" xml:base="http://localhost:4000/2021/11/03/umar.html"><![CDATA[<p>Modern systems need to handle a variety of workloads and use cases. It is very difficult for one system architecture to cater to these use cases therefore, a “one-size-fits-all” architecture does not work in practice. Even with specialized architectures, customers must make compromises on performance, functionality, and usability when running “hybrid” workloads. Further, after the initial deployment, customer’s workloads and requirements may change.</p>

<p>Over the last several years, at Microsoft, we have been on a quest to design practical instance optimized systems (IOS), systems which are custom-fit or “tailored” to customer’s initial requirements, and continuously “adapt” to their changing requirements. A key insight is that by “learning” from the specific data and workload distributions, a system can instance optimize itself to that specific data and workload. I’ll talk about the enabling trends for instance optimization and present our work on instance optimized indexes and storage layouts. I’ll conclude with some open research challenges.</p>

<p>Speaker bio:</p>

<p>Umar is a Principal Researcher in the Data Systems Group at Microsoft Research, Redmond. He currently works on instance optimized systems – applying ML to systems, and on improving price-performance for cloud-based transactional and analytics platforms.</p>]]></content><author><name>Dr. Umar Farooq Minhas (Microsoft Research)</name></author><summary type="html"><![CDATA[Modern systems need to handle a variety of workloads and use cases. It is very difficult for one system architecture to cater to these use cases therefore, a “one-size-fits-all” architecture does not work in practice. Even with specialized architectures, customers must make compromises on performance, functionality, and usability when running “hybrid” workloads. Further, after the initial deployment, customer’s workloads and requirements may change.]]></summary></entry><entry><title type="html">Hydra: A Data System for Large Multi-Model Deep Learning</title><link href="http://localhost:4000/2021/10/27/kabir.html" rel="alternate" type="text/html" title="Hydra: A Data System for Large Multi-Model Deep Learning" /><published>2021-10-27T00:00:00-05:00</published><updated>2021-10-27T00:00:00-05:00</updated><id>http://localhost:4000/2021/10/27/kabir</id><content type="html" xml:base="http://localhost:4000/2021/10/27/kabir.html"><![CDATA[<p>Recent advances in deep learning (DL) architectures have improved model quality in a variety of domains, but have come at the expense of a substantial increase in model sizes. Model architectures have reached unprecedented scales, and multi-billion parameter models have become commonplace.</p>

<p>Unfortunately, GPU memory capacity growth has not kept pace with this rapid expansion, introducing a new systems bottleneck for DL users. While solutions exist, such as model parallelism, they suffer from performance drawbacks and introduce efficiency issues of their own.</p>

<p>In this talk, I will introduce Hydra, our new system for large-scale multi-model training. Hydra takes a fresh database-inspired approach to the problem, introducing new ML systems techniques adapted from RDBMSs. This includes a highly general form of automatic model sharding and spilling across memory hierarchy, a novel hybrid of model parallelism and task parallelism inspired by multi-query optimization, and a new paired scheduling scheme inspired by double buffering in RDBMSs.</p>

<p>Our system demonstrates linear speedups against model parallelism and 50% faster training times versus state-of-the-art pipeline parallel techniques. To explore Hydra’s real-world usability, we are currently working with a group training deep learning models for physics, where high-resolution data poses challenges for scalability.</p>

<p>Speaker bio:</p>

<p>Kabir is a Ph.D. student in the UC San Diego databases group advised by Professor Arun Kumar. His research broadly addresses performance optimizations in machine learning systems design, with his most recent work focusing on enabling efficient training of large-scale models.</p>]]></content><author><name>Kabir Nagrecha (UC San Diego)</name></author><summary type="html"><![CDATA[Recent advances in deep learning (DL) architectures have improved model quality in a variety of domains, but have come at the expense of a substantial increase in model sizes. Model architectures have reached unprecedented scales, and multi-billion parameter models have become commonplace.]]></summary></entry><entry><title type="html">Accelerating Analytic Queries on Oracle In-Memory Database</title><link href="http://localhost:4000/2021/10/18/weiwei.html" rel="alternate" type="text/html" title="Accelerating Analytic Queries on Oracle In-Memory Database" /><published>2021-10-18T00:00:00-05:00</published><updated>2021-10-18T00:00:00-05:00</updated><id>http://localhost:4000/2021/10/18/weiwei</id><content type="html" xml:base="http://localhost:4000/2021/10/18/weiwei.html"><![CDATA[<p>Oracle In-Memory database was first released in 2014, with its unique dual-format architecture, Oracle Database In-Memory transparently accelerates analytics queries by orders of magnitude, and enables HTAP for real time decision making. On columnar data, the scan could be blazingly fast, however the challenge remains on improving important higher level SQL operators such as joins and aggregation performance. In this talk, I will first introduce Oracle Database In-Memory architecture, then I will present various work we have made as well as ongoing work on accelerating In-Memory analytic queries, such as novel data structures and algorithms design, query execution engine re-design, and modern hardware acceleration.</p>

<p>Speaker bio:</p>

<p>Weiwei Gong is the Senior Manager of Data and In-Memory Technologies at Oracle. She is interested in the intersection of data management, high-performance computing and computer systems. Passionate about hardware software co-design, Weiwei leads a team of talented developers that builds performance-critical features of Oracle Database In-Memory. Her work has enabled efficient analytic query processing by leveraging emerging hardware technologies. Weiwei earned her M.S. from Renmin University of China, and Ph.D. from UMass Boston, both in Computer Science.</p>]]></content><author><name>Dr. Weiwei Gong (Oracle)</name></author><summary type="html"><![CDATA[Oracle In-Memory database was first released in 2014, with its unique dual-format architecture, Oracle Database In-Memory transparently accelerates analytics queries by orders of magnitude, and enables HTAP for real time decision making. On columnar data, the scan could be blazingly fast, however the challenge remains on improving important higher level SQL operators such as joins and aggregation performance. In this talk, I will first introduce Oracle Database In-Memory architecture, then I will present various work we have made as well as ongoing work on accelerating In-Memory analytic queries, such as novel data structures and algorithms design, query execution engine re-design, and modern hardware acceleration.]]></summary></entry><entry><title type="html">Self-Driving Database Management Systems: Forecasting, Modeling, And Planning</title><link href="http://localhost:4000/2021/10/06/lin.html" rel="alternate" type="text/html" title="Self-Driving Database Management Systems: Forecasting, Modeling, And Planning" /><published>2021-10-06T00:00:00-05:00</published><updated>2021-10-06T00:00:00-05:00</updated><id>http://localhost:4000/2021/10/06/lin</id><content type="html" xml:base="http://localhost:4000/2021/10/06/lin.html"><![CDATA[<p>Database management systems (DBMSs) are an important part of modern data-driven applications. However, they are notoriously difficult to deploy and administer because they have many aspects that one can change that affect their performance, including database physical design and system configuration. There are existing methods that recommend how to change these aspects of databases for an application. But most of them require humans to make final decisions on what changes to apply and when to apply them. Furthermore, these previous tuning methods either (1) require expensive exploratory testing, (2) are reactionary to the workload and can only solve problems after they occur, (3) focus only on improving one single aspect of the DBMS, or (4) do not provide explanations on their decisions. Thus, most DBMSs today still require onerous and costly human administration.</p>

<p>We present a novel architecture for a self-driving DBMS that enables automatic system management and removes the administration impediments. Our approach consists of three frameworks: (1) workload forecasting, (2) behavior modeling, and (3) action planning. The workload forecasting framework predicts the query arrival rates under varying database workload patterns using an ensemble of time-series forecasting models. The behavior modeling framework constructs fine-grained machine learning models that predict the runtime behavior of the DBMS. Lastly, the action planning framework generates a sequence of optimization actions based on these forecasted workload patterns and behavior model estimations. It uses receding horizon control and Monte Carlo tree search to approximate the complex optimization problem effectively.</p>

<p>Our forecasting-modeling-planning architecture enables an autonomous DBMS that proactively plans for optimization actions without expensive testing. It automatically applies the actions at proper times, holistically controls all system aspects, and provides explanations on its decisions.</p>

<p>Speaker bio:</p>

<p>Lin is a Postdoc in the CMU database group working with Andy Pavlo to develop the NoisePage DBMS, a self-driving DBMS designed from the ground up. Lin recently graduated from CMU with a PhD in Computer Science.</p>

<p>Public video of talk: https://www.youtube.com/watch?v=2rvv_cBK0uY</p>]]></content><author><name>Dr. Lin Ma (CMU)</name></author><summary type="html"><![CDATA[Database management systems (DBMSs) are an important part of modern data-driven applications. However, they are notoriously difficult to deploy and administer because they have many aspects that one can change that affect their performance, including database physical design and system configuration. There are existing methods that recommend how to change these aspects of databases for an application. But most of them require humans to make final decisions on what changes to apply and when to apply them. Furthermore, these previous tuning methods either (1) require expensive exploratory testing, (2) are reactionary to the workload and can only solve problems after they occur, (3) focus only on improving one single aspect of the DBMS, or (4) do not provide explanations on their decisions. Thus, most DBMSs today still require onerous and costly human administration.]]></summary></entry><entry><title type="html">Programmatically Building &amp;amp; Managing Training Data with Snorkel</title><link href="http://localhost:4000/2020/12/04/alex.html" rel="alternate" type="text/html" title="Programmatically Building &amp;amp; Managing Training Data with Snorkel" /><published>2020-12-04T00:00:00-06:00</published><updated>2020-12-04T00:00:00-06:00</updated><id>http://localhost:4000/2020/12/04/alex</id><content type="html" xml:base="http://localhost:4000/2020/12/04/alex.html"><![CDATA[<p>One of the key bottlenecks in building machine learning systems is creating and managing the massive training datasets that today’s models require. In this talk, I will describe our work on Snorkel (snorkel.org), an open-source framework for building and managing training datasets, and describe three key operators for letting users build and manipulate training datasets: labeling functions, for labeling unlabeled data; transformation functions, for expressing data augmentation strategies; and slicing functions, for partitioning and structuring training datasets.  These operators allow domain expert users to specify machine learning (ML) models entirely via noisy operators over training data, expressed as simple Python functions—or even via higher level NL or point-and-click interfaces—leading to applications that can be built in hours or days, rather than months or years, and that can be iteratively developed, modified, versioned, and audited. I will describe recent work on modeling the noise and imprecision inherent in these operators, and using these approaches to train ML models that solve real-world problems, including recent state-of-the-art results on benchmark tasks and real-world industry, government, and medical deployments.</p>

<p>Speaker bio:</p>

<p>Alex Ratner is the co-founder and CEO of Snorkel AI, Inc., which supports the open source Snorkel library and develops Snorkel Flow, an end-to-end system for building machine learning applications, and an Assistant Professor of Computer Science at the University of Washington.  Prior to Snorkel AI and UW, he completed his PhD in CS advised by Christopher Ré at Stanford, where his research focused on applying data management and statistical learning techniques to emerging machine learning workflows, such as creating and managing training data, and applying this to real-world problems in medicine, knowledge base construction, and more.</p>

<p>Public video of talk: https://www.youtube.com/watch?v=fGBGSoW-g-E</p>]]></content><author><name>Dr. Alex Ratner (UW-Seattle and Snorkel AI)</name></author><summary type="html"><![CDATA[One of the key bottlenecks in building machine learning systems is creating and managing the massive training datasets that today’s models require. In this talk, I will describe our work on Snorkel (snorkel.org), an open-source framework for building and managing training datasets, and describe three key operators for letting users build and manipulate training datasets: labeling functions, for labeling unlabeled data; transformation functions, for expressing data augmentation strategies; and slicing functions, for partitioning and structuring training datasets. These operators allow domain expert users to specify machine learning (ML) models entirely via noisy operators over training data, expressed as simple Python functions—or even via higher level NL or point-and-click interfaces—leading to applications that can be built in hours or days, rather than months or years, and that can be iteratively developed, modified, versioned, and audited. I will describe recent work on modeling the noise and imprecision inherent in these operators, and using these approaches to train ML models that solve real-world problems, including recent state-of-the-art results on benchmark tasks and real-world industry, government, and medical deployments.]]></summary></entry><entry><title type="html">Responsible Data Management</title><link href="http://localhost:4000/2020/11/20/julia.html" rel="alternate" type="text/html" title="Responsible Data Management" /><published>2020-11-20T00:00:00-06:00</published><updated>2020-11-20T00:00:00-06:00</updated><id>http://localhost:4000/2020/11/20/julia</id><content type="html" xml:base="http://localhost:4000/2020/11/20/julia.html"><![CDATA[<p>The need for responsible data management intensifies with the growing impact of data on society. One central locus of the societal impact of data are Automated Decision Systems (ADS), socio-legal-technical systems that are used broadly in industry, non-profits, and government. ADS process data about people, help make decisions that are consequential to people’s lives, are designed with the stated goals of improving efficiency and promoting equitable access to opportunity, involve a combination of human and automated decision making, and are subject to auditing for legal compliance and to public disclosure. They may or may not use AI, and may or may not operate with a high degree of autonomy, but they rely heavily on data.</p>

<p>In this talk I hope to convince you that the data management community should play a central role in the responsible design, development, use, and oversight of ADS. I outline a technical research agenda and also argue that, to make progress, we may need to step outside our engineering comfort zone and start reasoning in terms of values and beliefs, in addition to checking results against known ground truths and optimizing for efficiency objectives. This seems high-risk, but one of the upsides is being able to explain to our children what we do and why it matters.</p>

<p>Speaker bio:</p>

<p>Julia Stoyanovich is an Assistant Professor of Computer Science and Engineering and of Data Science at New York University.  Julia’s research focuses on responsible data management and analysis: on operationalizing fairness, diversity, transparency, and data protection in all stages of the data science lifecycle. She is the founding director of the Center for Responsible AI at NYU, a comprehensive  laboratory that is building a future in which responsible AI will be the only kind accepted by society.  Julia is developing and  teaching courses on responsible data science at NYU, and is the co-creator of an award-winning comic on this topic (https://dataresponsibly.github.io/comics/). In addition to data ethics, she works on management and analysis of preference data, and on querying large evolving graphs.  Julia holds M.S. and Ph.D. degrees in Computer Science from Columbia University, and a B.S. in Computer Science and in Mathematics and Statistics from the University of Massachusetts at Amherst. Julia is a recipient of an NSF CAREER award and of an NSF/CRA CI Fellowship.</p>

<p>Public video of talk: https://www.youtube.com/watch?v=57c025_xXdI</p>]]></content><author><name>Dr. Julia Stoyanovich (NYU)</name></author><summary type="html"><![CDATA[The need for responsible data management intensifies with the growing impact of data on society. One central locus of the societal impact of data are Automated Decision Systems (ADS), socio-legal-technical systems that are used broadly in industry, non-profits, and government. ADS process data about people, help make decisions that are consequential to people’s lives, are designed with the stated goals of improving efficiency and promoting equitable access to opportunity, involve a combination of human and automated decision making, and are subject to auditing for legal compliance and to public disclosure. They may or may not use AI, and may or may not operate with a high degree of autonomy, but they rely heavily on data.]]></summary></entry><entry><title type="html">Systems for Human Data Interaction</title><link href="http://localhost:4000/2020/11/13/eugene.html" rel="alternate" type="text/html" title="Systems for Human Data Interaction" /><published>2020-11-13T00:00:00-06:00</published><updated>2020-11-13T00:00:00-06:00</updated><id>http://localhost:4000/2020/11/13/eugene</id><content type="html" xml:base="http://localhost:4000/2020/11/13/eugene.html"><![CDATA[<p>The rapid democratization of data has placed its access and analysis in the hands of the entire population. While the advances in rapid and large-scale data processing continue to reduce runtimes and costs, the interfaces and tools for end-users to interact with, and work with, data is still lacking. It is still too difficult to translate a user’s data needs into the appropriate interfaces, too difficult to develop data intensive interfaces that are responsive and scalable, and too difficult for users to understand and interpret the data they see. In this talk, I will provide an overview of our lab’s recent work on systems for human data interaction that go towards addressing these challenges.</p>

<p>Speaker bio:</p>

<p>Eugene Wu is an Associate Professor of Computer Science at Columbia University. He received a Ph.D. in EECS from MIT in 2014, and B.S. from UC Berkeley. He is broadly interested in technologies for human data interaction. His goal is for users at all technical levels to effectively and quickly make sense of their information. Eugene is interested in solutions that ultimately improve the interface between users and data. He combines his background in database management systems with techniques from crowd-sourcing, visualization, and HCI. Eugene Wu has received the VLDB 2018 10-year test of time award, best-of-conference citations at ICDE and VLDB, the SIGMOD 2016 best demo award, the NSF CAREER, and the Google and Amazon faculty awards.</p>]]></content><author><name>Dr. Eugene Wu (Columbia University)</name></author><summary type="html"><![CDATA[The rapid democratization of data has placed its access and analysis in the hands of the entire population. While the advances in rapid and large-scale data processing continue to reduce runtimes and costs, the interfaces and tools for end-users to interact with, and work with, data is still lacking. It is still too difficult to translate a user’s data needs into the appropriate interfaces, too difficult to develop data intensive interfaces that are responsive and scalable, and too difficult for users to understand and interpret the data they see. In this talk, I will provide an overview of our lab’s recent work on systems for human data interaction that go towards addressing these challenges.]]></summary></entry></feed>